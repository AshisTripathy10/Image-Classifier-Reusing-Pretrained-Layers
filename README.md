Introduction: In this project, we train a neural network from scratch to classify data using TensorFlow 2 while also leveraging the weights from an already trained model for classification on a different dataset. Our objective is to use the MNIST dataset to identify various fashion products. We begin by importing the keras module from the TensorFlow library, which offers a suite of tools and functions essential for constructing and training neural network models.

Dividing the Datasets: We train a model on dataset A, which includes a classification task with eight classes, and attempt to apply the learned model to dataset B for binary classification ((y[y_5_or_6] == 6)). Our approach is rooted in the hope of transferring knowledge from dataset A to B, leveraging the conceptual similarities between items such as sneakers, ankle boots, coats, and t-shirts in set A and sandals and shirts in set B. However, our current model utilizes Dense layers, which process input data as a flat array where spatial relationships between features are not maintained, meaning they can only recognize patterns if they appear in the exact positions as learned during training. This contrasts with convolutional layers, which use filters to scan through the entire image, allowing them to detect features regardless of their position and thereby preserving the spatial hierarchy of the input data.

Building the Model: We develop a sequential neural network, model_A, using TensorFlow's Keras API, which begins with a Flatten layer converting 28×28 pixel images into 1D arrays. Following this, a series of Dense layers with descending neuron counts (300, 100, 50, 50, 50) reduce complexity and risk of overfitting. Each layer employs the "selu" activation function, stabilizing variance and managing gradient magnitudes to prevent the vanishing gradient problem and accelerate convergence. The final layer, with 8 neurons and a "softmax" activation, is tailored for multi-class classification. The model, compiled with the sparse_categorical_crossentropy loss and optimized by SGD at a learning rate of 0.001, aims for high accuracy and effective generalization across 8 classes.

Training the Model: The training of neural network model model_A over five epochs started with an initial training accuracy of 74.98% and ended at 88.69%. Validation accuracy similarly improved from 84.90% to 88.84%. Both training and validation losses decreased progressively, demonstrating effective learning. We save the trained neural network model model_A to a file named my_model_A.h5, which stores the model’s architecture, weights, and training configuration in the HDF5 format, allowing for later retrieval and reuse 

Building Model B: We build model_B, which has a similar architecture to model_A but is adapted for binary classification. It features a Flatten layer followed by Dense layers with "selu" activation, concluding with a single output unit with "softmax" activation. Unlike model_A, which is designed for multi-class classification with 8 output units, model_B uses binary_crossentropy loss to handle two classes. We trained model model_B for five epochs, reducing loss from 0.2678 to 0.0335. Despite this decrease, accuracy remained at 50% for both training and validation. This suggests that while the model’s predicted probabilities improved, it did not effectively translate these probabilities into correct classifications.

Creating a new Model based on existing model A: We create model_B_on_A by cloning model_A and modifying it for binary classification. The new model retains the original layers from model_A but replaces the final layer with a new Dense layer having a single output unit with "sigmoid" activation. All layers except the final one are set to be non-trainable. The summary of the model shows that model_B_on_A has a total of 275,801 parameters, with only 51 being trainable (in the new output layer), and the remaining 275,750 parameters are non-trainable. We compiled model_B_on_A with binary_crossentropy loss and SGD optimizer, then trained it for five epochs. The model demonstrated significant improvement, with loss decreasing from 0.3751 to 0.1033 and accuracy rising from 83.18% to 98.56% on the training data. Validation metrics also improved, with loss decreasing from 0.2783 to 0.1057 and accuracy increasing from 91.38% to 98.48%.

Evaluating the Models: We evaluate the models on test data with model_B_on_A achieving 98.86% accuracy, significantly outperforming model_B, which reached 49.84%. The superior performance of model_B_on_A is attributed to transfer learning from pre-trained features and the use of a sigmoid activation function for binary classification. In contrast, model_B, which uses a softmax activation function, lacks the benefit of transfer learning and is less effective for binary classification tasks.

